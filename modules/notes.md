## Module 1: The different types of Machine Learning

Depending of the type of data available, and the kind of problem you want to solve, you'll probably select one of two Machine Learning types:

  - <ins>Supervised Machine Learning</ins>: Uses labeled datasets to train algorithms to classify or predict outcomes. Supervised learning problems
occur more frequently in the workplace, so data professionals use this type most often.
    To summarize, supervised machine learning algorithms use data with answers already in it.

  - <ins>Unsupervised Machine Learning</ins>: Uses algorithms to analyze and cluster unlabeled datasets.

Generally, there are 3 types of Python packages:

  - Operational packages: They load, structure and prepare the dataset for further analysis (E.g., Pandas, Numpy, Scipy).
  - Data Visualization packages: They allow you to create plots and graphics of data (E.g., Matplotlib, Seaborn, Plotly).
  - Machine Learning packages: They give many functions to help build models from a dataset, along with functionality to examine the model once it has
been built (E.g., Sklearn).

Apart from these, Python has thousands and thousands of packages publicly available.

## Module 2: Workflow for building complex models

  - <ins>Feature Engineering</ins>: The process of using practical, statistical, and data science knowledge to select, transform, or extract
characteristics, properties, and attributes from raw data.

  Your process for feature engineering will be changing and altering variables (Continuous/Categorical) with the end goal of using them to train a
model.

  The three general categories of feature engineering are Selection, Transformation and Extraction.

  - <ins>Class Imbalance</ins>: When a dataset has a predictor variable that contains more instances of one outcome than another.

Balancing a dataset:

    - Downsampling: Downsampling is the process of making the minority class represent a larger share of the whole dataset
    simply by removing observations from the majority class. It is mostly used with datasets that are large.
    
    - Upsampling: Upsampling is basically the opposite of downsampling, and is done when the dataset doesnâ€™t have a very large
    number of observations in the first place. Instead of removing observations from the majority class, you increase the number of
    observations in the minority class.

Class rebalancing should be reserved for situations where other alternatives have been exhausted and you still are not achieving satisfactory model
results.

Continuous, Discrete, Categorical.






